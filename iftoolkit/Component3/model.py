from __future__ import annotations
from typing import Dict, List, Optional, Any
import numpy as np
import pandas as pd
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import RandomForestClassifier
from .Fairness import FairnessPipeline   
from .plots import get_plots
from sklearn.base import clone


def ensure_probabilistic_estimator(estimator, *, method: str = "isotonic", cv: int = 3):
    """
    Return an estimator with `predict_proba`.

    If `estimator` already implements `predict_proba`, return it unchanged;
    otherwise wrap with `CalibratedClassifierCV(method, cv)` to obtain calibrated
    probabilities. See docs: docs/model.md#ensure_probabilistic_estimator.
    """
    if hasattr(estimator, "predict_proba"):
        return estimator

    return CalibratedClassifierCV(estimator=estimator, method=method, cv=cv)


class Model:
    """
    Estimator-agnostic fairness facade.

    Wraps the pipeline with a simple, stateful object. Accepts any sklearn
    classifier for the outcome model (and, in DR mode, for the propensity model).
    Provides: preprocessing, fitting (SR/DR), summaries, and plots.
    See docs: docs/model.md#class-model
    """

    def __init__(
        self,
        data: pd.DataFrame,
        model_type: str = "rf",                        
        *,
        outcome_estimator: Optional[object] = None,    
        propensity_estimator: Optional[object] = None,  
        treatment: Optional[str] = None,
        outcome: Optional[str] = None,
        covariates: Optional[List[str]] = None,
        protected_characteristics: tuple = (),
        risk_score: Optional[str] = None,
        treatment_flag: bool = True,
        group_label_map: Optional[dict] = None,
        coeff_map: Optional[dict] = None,
        random_state: int = 42,
        n_splits: int = 5,
        method: str = "sr",             #'sr' or 'dr'
        auto_compute_propensity: bool = True,
        calibration_method: str = "isotonic",
        calibration_cv: int = 3,
    ):
        """
        Initialize configuration and hold data.

        Most users set: outcome, protected_characteristics, covariates,
        and either outcome_estimator or model_type. For DR, optionally
        supply propensity_estimator (or let it auto-compute).
        See docs: docs/model.md#__init__
        """
        self.data = data.copy()
        self.Y = outcome
        self.D = treatment
        self.covariates = list(covariates) if covariates is not None else []
        self.model_type = model_type
        self.A1 = protected_characteristics[0] if len(protected_characteristics) > 0 else None
        self.A2 = protected_characteristics[1] if len(protected_characteristics) > 1 else None
        self.S_prob = risk_score
        self.treatment_flag = treatment_flag
        self.group_label_map = group_label_map
        self.coeff_map = coeff_map

        #Internals
        self.A = "A1A2"
        self.results_: Optional[Dict[str, object]] = None

        #Pipeline config
        self._outcome_estimator = outcome_estimator  #can be None → pipeline uses model_type factory
        self._propensity_estimator = propensity_estimator
        self._random_state = random_state
        self._n_splits = n_splits
        self._method = method
        self._auto_compute_propensity = auto_compute_propensity
        self._calibration_method = calibration_method
        self._calibration_cv = calibration_cv

    #---------- basics pre-processing ----------
    def pre_process_data(self) -> None:
        if self.Y is None or self.Y not in self.data.columns:
            raise ValueError("Outcome column 'Y' must be provided and present in data.")
        self.data[self.Y] = self.data[self.Y].astype('category')

        if self.D is not None and self.D in self.data.columns:
            self.data[self.D] = self.data[self.D].astype('category')

        if not self.A1 or not self.A2:
            raise ValueError("protected_characteristics must provide (A1, A2).")

        if self.A1 not in self.data.columns or self.A2 not in self.data.columns:
            raise ValueError("A1 or A2 not found in data columns.")

        #intersectional label
        self.data[self.A] = self.data[self.A1].astype(str) + self.data[self.A2].astype(str)

        #default covariates if not provided: numeric columns except protected + Y + D + A1A2
        if not self.covariates:
            drop_cols = {self.Y, self.A1, self.A2, self.A}
            if self.D is not None:
                drop_cols.add(self.D)
            self.covariates = [
                c for c in self.data.columns
                if c not in drop_cols and self.data[c].dtype != 'O'
            ]

    def _ensure_dr_inputs(self) -> None:
        """
        In DR mode, ensure π_g(X) columns exist or auto-fit them.

        If method=='dr' and columns like group_<g>_prob are missing, fit a
        multiclass propensity model and add them. No-op otherwise.
        See docs: docs/model.md#_ensure_dr_inputs
        """
        if self._method != "dr":
            return

        #determine which columns should exist
        groups = self.data[self.A].astype(str).unique().tolist()
        missing = [g for g in groups if f"group_{g}_prob" not in self.data.columns]
        if not missing:
            return

        if not self._auto_compute_propensity:
            missing_cols = [f"group_{g}_prob" for g in missing]
            raise ValueError(
                f"DR mode selected but missing propensity columns: {missing_cols}. "
                f"Either provide them or set auto_compute_propensity=True."
            )

        #Fit π_g(X) and append group_<g>_prob columns
        self.data = Model.add_group_propensities_general(
            df=self.data,
            covariates=self.covariates,
            group_col=self.A,
            estimator=self._propensity_estimator,
            random_state=self._random_state,
            calibration_method=self._calibration_method,
            calibration_cv=self._calibration_cv,
        )


    @staticmethod
    def add_group_propensities_general(
        df: pd.DataFrame,
        covariates: List[str],
        group_col: str = "A1A2",
        estimator: Optional[object] = None,
        random_state: int = 42,
        calibration_method: str = "isotonic",
        calibration_cv: int = 3,
    ) -> pd.DataFrame:
        """
        Fit π_g(X)=P(A=g|X) with any sklearn classifier and add columns.

        Writes one column per class: group_<g>_prob. Wraps non-probabilistic
        estimators with calibration. Returns a copy of df with added columns.
        See docs: docs/model.md#add_group_propensities_general
        """
        out = df.copy()
        X = out[covariates].to_numpy()
        y = out[group_col].astype(str).to_numpy()

        #default multiclass estimator if none provided
        base = estimator if estimator is not None else RandomForestClassifier(
            n_estimators=400, max_depth=None, random_state=random_state, n_jobs=-1
        )
        clf = ensure_probabilistic_estimator(
            clone(base), method=calibration_method, cv=calibration_cv
        )

        clf.fit(X, y)
        probs = clf.predict_proba(X)
        classes = np.asarray(clf.classes_, dtype=str)

        for j, g in enumerate(classes):
            out[f"group_{g}_prob"] = probs[:, j]

        return out


    #---------- Model info ----------
    def get_model_info(self) -> Dict[str, Any]:
        """
        Return a readable snapshot of configuration and data shape.

        Useful for logs and quick inspection (models used, method, columns, sizes).
        See docs: docs/model.md#get_model_info
        """
        return {
            "outcome_model": (
                f"custom({type(self._outcome_estimator).__name__})"
                if self._outcome_estimator is not None else self.model_type
            ),
            "propensity_model": (
                None if self._method != "dr" else (
                    f"custom({type(self._propensity_estimator).__name__})"
                    if self._propensity_estimator is not None else "default+calibration"
                )
            ),
            "method": self._method,
            "outcome": self.Y,
            "treatment": self.D,
            "protected_A1": self.A1,
            "protected_A2": self.A2,
            "group_col": self.A,
            "covariates": list(self.covariates),
            "risk_score_col": self.S_prob,
            "n_rows": int(len(self.data)),
            "n_covariates": int(len(self.covariates)),
            "n_splits": self._n_splits,
            "random_state": self._random_state,
            "auto_compute_propensity": self._auto_compute_propensity,
            "calibration": {"method": self._calibration_method, "cv": self._calibration_cv},
        }

    #---------- run fairness ----------
    def fit_fairness(
        self,
        cutoff: Optional[float] = None,
        gen_null: bool = True,
        R_null: int = 200,
        bootstrap: str = "rescaled",
        B: int = 500,
        m_factor: float = 0.75
    ) -> Dict[str, object]:
        """
        Run the full pipeline (SR/DR): cross-fit μ, choose τ, compute defs.

        Optionally builds a permutation null and/or rescaled bootstrap.
        Returns the pipeline results dict. See docs: docs/model.md#fit_fairness
        """
        if self.A not in self.data.columns:
            raise RuntimeError("Call pre_process_data() before fit_fairness().")

        #Ensure DR inputs if needed (this will fit π_g with any estimator)
        self._ensure_dr_inputs()

        pipe = FairnessPipeline(
            group_col=self.A,
            outcome_col=self.Y,
            covariates=self.covariates,
            estimator=self._outcome_estimator,   
            model_type=self.model_type,
            n_splits=self._n_splits,
            random_state=self._random_state,
            method=self._method
        )
        self.results_ = pipe.fit(
            data=self.data,
            cutoff=cutoff,
            gen_null=gen_null,
            R_null=R_null,
            bootstrap=bootstrap,
            B=B,
            m_factor=m_factor
        )
        return self.results_

    def summarize(self) -> pd.DataFrame:
        """
        Return key statistics as a tidy table (stat, value).

        Includes aggregate disparities and per-group cFPR/cFNR and observed FPR/FNR.
        See docs: docs/model.md#summarize
        """
        if self.results_ is None:
            raise RuntimeError("Call fit_fairness() before summarize().")
        return pd.DataFrame(list(self.results_["defs"].items()), columns=["stat", "value"])

    def plots(
        self,
        alpha: float = 0.05,
        m_factor: float = 0.75,
        delta_uval: float = 0.05
    ):
        """
        Assemble plot data, optional figures, and u-values.

        Returns (est_summaries, table_null_delta, table_uval). Figures render
        when a matplotlib backend is active. See docs: docs/model.md#plots
        """
        
        if self.results_ is None:
            raise RuntimeError("Call fit_fairness() before plots().")
        return get_plots(
            results=self.results_,
            sampsize=len(self.results_.get('est_choice', [])),
            alpha=alpha,
            m_factor=m_factor,
            delta_uval=delta_uval
        )



